---
title: 'A Spark, a Halt, and a Re-ignition: The History of Neural Networks'
date: '03-26-2024'
tags: ['history', 'neural-networks']
draft: false
summary: Neural networks, inspired by the brain, have a fascinating history. Born in the 1940s, they faced a cold spell in the following decades due to limitations and lack of computing power. The 1980s saw a revival...
---

# Let's learn a little about the history of **Neural Networks**?

![Neural Network Milestones](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F19288618%2Fad20e6b7f436d15275bc2749a1cbd5b8%2FMilestones.jpg?generation=1709708176889797&alt=media)
**Font:** [Deep Learning Book](https://www.deeplearningbook.com.br/uma-breve-historia-das-redes-neurais-artificiais/)

Inspired by the intricate workings of the human brain, the concept of **artificial neural networks** (ANNs) emerged in the 1940s. Warren McCulloch - a neurophysiologist - and Walter Pitts - a mathematician - laid the groundwork with their 1943 paper, introducing a simple model of an artificial neuron based in threshold logic algorithms. Then, in 1958, Frank Rosenblatt, intrigued by how a fly's eye works, developed the **Perceptron**, which many of you may know as first machine learning algorithm capable of learning.

![Perceptron](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F19288618%2Ff238cbdbf2cde644c6ba964e186b8c6a%2FPerceptron.png?generation=1709708397180707&alt=media)
**Font:** [Towards Data Science](https://towardsdatascience.com/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a)

Althought the idea of neural networks seemed very promising, they faced a significant setback, known as the "AI Winter" during 1960-1980. This was caused by two main factors:

- **Limitations of the Perceptron:** Marvin Minsky and Seymour Papert, in their influential 1969 book "Perceptrons," demonstrated that the Perceptron, with its limited architecture, could not solve many fundamental problems. This led to a decline in funding and interest in the field.
- **Computational limitations:** Training complex neural networks required significant computational power, which was not available at the time.

The 1980s witnessed a resurgence in ANN research. It all started with John Hopfield, a charismatic researcher from Caltech, who convinced the scientific community that Neural Networks were much more than modelling human brains. They were supposed to create useful devices.

In 1985, the American Institute of Physics began what became an annual meeting â€“ Neural Networks for Computation. In 1987, the first International Conference on Neural Networks of IEEE attracted over 1800 participants.

In 1986, three independent groups of researchers, including David Rumelhart, a former member of the Stanford psychology department, presented similar ideas that are now called **Backpropagation networks**, because they distribute pattern recognition errors throughout the network. Due to that, they learn more slowly, but they generate a very accurate result.

Nowadays, the Neural Networks are used in a variety of applications. From image recognition to natural language processing, speech recognition, recommendation systems - yeah, the ones from Netflix, Amazon and these other big companies. It's an universe of possibilities!

Here we have a cheat sheet containing many of Neural Network architectures developed since their ascension.

![A most complete chart of Neural Networks](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F19288618%2F84bf96c99407465f41e1b927059df36f%2FChart.png?generation=1709708436253136&alt=media)
**Font:** [Deep Learning Zoo](https://www.asimovinstitute.org/neural-network-zoo/)
